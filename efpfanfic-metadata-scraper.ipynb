{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EFP Fanfic Metadata Scraper\n",
    "This notebook scrapes metadata from the Italian fanfic site [EFP Fanfic](https://efpfanfic.net/). To make it work, put the URL for a particular fandom page (everything up to `pagina=`) in as the *ScraperStem* value below, and set the range to be (1,some-number), where some-number is the final page of the paginated results for that fandom."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import libraries\n",
    "import requests\n",
    "import xml.etree.ElementTree\n",
    "import csv\n",
    "import urllib.parse\n",
    "from random import randint\n",
    "import time\n",
    "from time import sleep\n",
    "import re\n",
    "import regex\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a Pandas dataframe with the metadata fields\n",
    "italianfanfic = pd.DataFrame(columns=[\"Title\", \"Storylink\", \"Color\", \"LastChap\", \"AuthName\", \"AuthID\", \"Published\", \"Updated\", \"Genre\", \"Chapters\", \"Status\", \"Shiptype\", \"Note\", \"Warning\", \"Characters\", \"Ships\", \"Contests\", \"Reviews\", \"Blurb\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's the cell you should modify with the fandom base URL and the range."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "headers = {'user-agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X x.y; rv:10.0) Gecko/20100101 Firefox/10.0 Chrome/74.0.3729.131'}\n",
    "#URL base, which is the page for a particular fandom, up to the place that indicates the page\n",
    "ScraperStem = \"https://efpfanfic.net/categories.php?catid=47&parentcatid=47&offset=15&pagina=\"\n",
    "#For each page in a particular range\n",
    "#(We know the total range by looking at the first page for the fandom, and seeing what # the last page of results is)\n",
    "for i in range(1172,3766):\n",
    "    #The full URL combines the base with the page number\n",
    "    ScraperURL = ScraperStem + str(i)\n",
    "    #Print the page\n",
    "    print(ScraperURL)\n",
    "    #Retrieve the page\n",
    "    page = requests.get(ScraperURL, headers=headers)\n",
    "    c = page.content\n",
    "    #Parse the page contents with Beautiful Soup\n",
    "    soup = BeautifulSoup(c, 'html.parser')\n",
    "    #Identify the container with the fics\n",
    "    fics = soup.find_all(\"div\", {\"class\": \"storybloc\"})\n",
    "    #For each fic\n",
    "    for fic in fics:\n",
    "        #Find the div with the title\n",
    "        title = fic.find('div', {'class': 'titlestoria'})\n",
    "        #Get the link around the title\n",
    "        storylink = title.a['href']\n",
    "        #Get the title text\n",
    "        titlename = title.text\n",
    "        #Find the title bar that has the color/rating\n",
    "        titlebar = fic.find('div', {'class': 'titlebloc'})\n",
    "        #Get the color/rating (ID value of the div below)\n",
    "        color = titlebar.find('div')\n",
    "        color = color.get('id')\n",
    "        #Look for the div that would indicate it's the last chapter\n",
    "        lastchap = titlebar.find('div', {'class': 'ultimochap'})\n",
    "        #If the last chapter div is not empty\n",
    "        if lastchap.text is not None:\n",
    "            #Then give lastchap a value\n",
    "            lastchap = 'lastchapter'\n",
    "            #Otherwise\n",
    "        else:\n",
    "            #Last chapter should be blank\n",
    "            lastchap = ''\n",
    "        #Find the blurb\n",
    "        blurb = fic.find('div', {'class': 'introbloc'}).text\n",
    "        #Find the metadata div\n",
    "        metadata = fic.find('div', {'class', 'notebloc'})\n",
    "        #If metadata isn't empty\n",
    "        if metadata is not None:\n",
    "            #Author ID has uid = [some number]\n",
    "            authid = re.findall(r'uid=([0-9]*)', str(metadata))\n",
    "            #Author name is the text inside the author ID link\n",
    "            authname = metadata.find('a').text\n",
    "            #Publication date comes after 'Pubblicata:'\n",
    "            published = re.search(r'Pubblicata: ((\\d\\/*)+) ', str(metadata)).group(1)\n",
    "            #Updated date comes after 'Aggiornata'\n",
    "            updated = re.search(r'Aggiornata: ((\\d\\/*)+) ', str(metadata)).group(1)\n",
    "            #Genre comes after 'Genere'\n",
    "            genre = re.search(r'Genere: (.*?\\|)', str(metadata))\n",
    "            #If genre isn't empty\n",
    "            if genre is not None:\n",
    "                #Capture genre value from regex\n",
    "                genre = genre.group(1)\n",
    "            #Chapters come after 'Capitoli'\n",
    "            chapters = re.search(r'Capitoli: (.*?\\<)', str(metadata))\n",
    "            #If chapters are not empty\n",
    "            if chapters is not None:\n",
    "                #Capture value of chapters\n",
    "                chapters = chapters.group(1)\n",
    "                chapters = re.search(r'(\\d*)', chapters).group(0)\n",
    "            #Capture the text after 'Capitoli' which should also include the status\n",
    "            status = re.search(r'Capitoli: (.*?\\<)', str(metadata))\n",
    "            #Refine the text to capture the actual status\n",
    "            if status is not None:\n",
    "                status = status.group(1)\n",
    "                status = re.search(r'(\\| )(.*)(<)$', status).group(2)\n",
    "            #Ship type comes after 'Tipo di coppia'\n",
    "            shiptype = re.search(r'Tipo di coppia: ((.*?)\\|)', str(metadata))\n",
    "            #Capture value\n",
    "            if shiptype is not None:\n",
    "                shiptype = shiptype.group(2)\n",
    "            #Note comes after text 'Note'\n",
    "            note = re.search(r'Note: ((.*?)\\|)', str(metadata))\n",
    "            #Capture note value\n",
    "            if note is not None:\n",
    "                note = note.group(2)\n",
    "            #Text warning comes after 'Avvertimenti'\n",
    "            textwarning = re.search(r'Avvertimenti: (.*)', str(metadata))\n",
    "            #Capture text warning value\n",
    "            if textwarning is not None:\n",
    "                textwarning = textwarning.group(1)\n",
    "            else:\n",
    "                #Or otherwise set it to blank\n",
    "                textwarning =''\n",
    "            #Characters come after Personaggi'\n",
    "            characters = re.search(r'Personaggi: (.*)', str(metadata))\n",
    "            #Capture character value\n",
    "            if characters is not None:\n",
    "                characters = characters.group(1)\n",
    "            #Ships come after 'Coppie'\n",
    "            ships = re.search(r'Coppie: (.*)', str(metadata))\n",
    "            #Capture ship value\n",
    "            if ships is not None:\n",
    "                ships = ships.group(1)\n",
    "            #Contest info comes after 'Contesto'\n",
    "            contest = re.search(r'Contesto: ((.*?)\\|)', str(metadata))\n",
    "            #Capture contest info\n",
    "            if contest is not None:\n",
    "                contest = contest.group(2)\n",
    "            #Reviews comes at the end before 'recension'\n",
    "            reviews = re.search(r'>(\\d+)</a> recension', str(metadata))\n",
    "            #Capture reviews value\n",
    "            if reviews is not None:\n",
    "                reviews = reviews.group(1)\n",
    "                #Create new item with the data that's been scraped\n",
    "                newitem = {\"Title\": title, \"Storylink\": storylink, \"Color\": color, \"LastChap\": lastchap, \"AuthName\": authname, \"AuthID\": authid, \"Published\": published, \"Updated\": updated, \"Genre\": genre, \"Chapters\": chapters, \"Status\": status, \"Shiptype\": shiptype, \"Note\": note, \"Warning\": textwarning, \"Characters\": characters, \"Ships\": ships, \"Contests\": contest, \"Reviews\": reviews, \"Blurb\": blurb}\n",
    "                #Append new item to the Pandas dataframe\n",
    "                italianfanfic = italianfanfic.append(newitem, ignore_index=True)\n",
    "    #Sleep 4-10 seconds before going to the next page\n",
    "    sleep(randint(4,10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Display the data\n",
    "italianfanfic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove newlines and tabs, then display the data again\n",
    "cleanitalianfanfic = italianfanfic.replace(to_replace=[r\"\\\\t|\\\\n|\\\\r\", \"\\t|\\n|\\r\"], value=[\" \",\" \"], regex=True, inplace=False)\n",
    "cleanitalianfanfic\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save the results to a TSV file\n",
    "cleanitalianfanfic.to_csv('/Users/qad/Documents/italianfanfic2021-2.tsv', index=False, sep=\"\\t\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
